{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d051cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chonkieNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading chonkie-1.3.1-cp310-cp310-win_amd64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\jayit\\codebasics_webinar\\venv\\lib\\site-packages (from chonkie) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jayit\\codebasics_webinar\\venv\\lib\\site-packages (from tqdm>=4.64.0->chonkie) (0.4.6)\n",
      "Downloading chonkie-1.3.1-cp310-cp310-win_amd64.whl (491 kB)\n",
      "Installing collected packages: chonkie\n",
      "Successfully installed chonkie-1.3.1\n"
     ]
    }
   ],
   "source": [
    "%pip install chonkie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c51021c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (768,)\n",
      "[-2.11551264e-02  1.86272599e-02  2.30103508e-02 -2.51132138e-02\n",
      "  6.03017472e-02  5.79888858e-02  3.07043549e-02 -3.16356272e-02\n",
      " -3.81117351e-02 -6.62747351e-03 -9.09226984e-02 -1.00578554e-02\n",
      "  2.66843960e-02  5.02246944e-03 -2.42901668e-02  1.02663226e-02\n",
      " -2.51493994e-02  1.12899207e-03 -1.71100609e-02  5.92033416e-02\n",
      "  7.68584982e-02 -5.68269799e-03  4.75457609e-02 -2.27421988e-03\n",
      "  8.70831963e-03  4.11179475e-03 -3.32887168e-03 -3.11696120e-02\n",
      "  1.75027531e-02  2.39923578e-02  4.78169620e-02 -8.04247633e-02\n",
      " -2.02872362e-02  9.30556841e-03 -8.65197508e-04 -4.14962834e-03\n",
      "  6.53738007e-02 -3.36869955e-02 -2.20747832e-02  8.39785263e-02\n",
      "  5.65880425e-02 -6.45676628e-03 -3.69528611e-03  2.63899993e-02\n",
      " -1.38421385e-02  9.04448237e-03  7.59396553e-02 -1.42448647e-02\n",
      "  4.55481820e-02  5.68690225e-02  2.73201168e-02  2.82014869e-02\n",
      " -1.01696569e-02 -2.81958561e-02 -3.92262749e-02  3.36293690e-03\n",
      " -3.52475494e-02  2.55262442e-02 -8.06037989e-03  2.40293182e-02\n",
      " -1.75328064e-03 -4.61043790e-02  1.30138457e-01  5.04820235e-03\n",
      "  6.76025171e-03  2.87002930e-03  1.34447049e-02 -1.75736304e-02\n",
      " -3.89624946e-02 -4.55863588e-02 -4.27390598e-02 -1.02810366e-02\n",
      " -2.12068651e-02  5.26441727e-03 -5.13345338e-02 -3.70311141e-02\n",
      "  8.77740886e-03 -4.63603288e-02  1.14428410e-02  5.11732176e-02\n",
      " -4.33431269e-05 -6.79150820e-02  7.32129738e-02  2.48144902e-02\n",
      " -2.10332982e-02  3.08383331e-02 -5.73651027e-03  2.39881296e-02\n",
      "  2.00637951e-02 -1.30535001e-02 -4.83075492e-02  1.42722204e-02\n",
      " -4.44994643e-02 -1.92969590e-02 -7.24921422e-03  1.07734906e-03\n",
      "  1.65345259e-02  2.01628469e-02  3.27760577e-02 -1.45508369e-04\n",
      "  3.63767520e-03 -1.73258781e-02 -1.90564524e-02 -1.24014281e-02\n",
      "  6.63027167e-03  7.24602258e-03  4.50634435e-02 -1.14035634e-02\n",
      "  3.21227238e-02  8.59025195e-02  5.18272910e-03  2.73008668e-03\n",
      " -8.40081200e-02  2.98720449e-02  3.63366790e-02  8.21760390e-03\n",
      "  7.46394247e-02 -1.15366704e-04 -1.61439274e-03  4.20858227e-02\n",
      "  2.62999777e-02 -8.39858204e-02  5.48356213e-02 -5.12181260e-02\n",
      "  2.44778842e-02  3.61126401e-02  5.72241955e-02 -4.84507680e-02\n",
      " -2.00795736e-02  3.56756411e-02 -1.15305278e-02  1.82046846e-03\n",
      " -7.91015923e-02 -5.90167427e-03  3.44506535e-03  5.44857271e-02\n",
      "  2.86928634e-03  4.61214744e-02  2.57049240e-02 -4.77143563e-02\n",
      "  4.33899015e-02  1.68524776e-02 -1.34690476e-04 -3.85370920e-04\n",
      "  2.80939322e-02 -1.89938396e-02  5.31356260e-02 -4.66950350e-02\n",
      "  3.76355872e-02 -4.23833393e-02 -1.13870045e-02  1.05594425e-02\n",
      " -8.40179548e-02  3.38178803e-03 -4.95811785e-03  3.22147645e-02\n",
      "  1.59245301e-02  4.68715765e-02 -6.70812326e-03 -4.23017703e-03\n",
      " -1.56976469e-02 -5.22370636e-02  1.14845306e-01  1.32483346e-02\n",
      "  5.74086681e-02 -3.78723582e-03 -7.57932104e-03  2.20359005e-02\n",
      " -1.03851333e-02  2.85539627e-02 -2.92632952e-02 -5.33712246e-02\n",
      " -1.17110452e-02  1.75133988e-03 -2.41425931e-02  4.01771404e-02\n",
      " -1.45348553e-02  3.16826552e-02 -8.61185119e-02  1.85845103e-02\n",
      " -1.87234115e-02  5.38356751e-02  1.74960457e-02  1.07187824e-03\n",
      " -4.99371588e-02  4.68855398e-03 -4.25082073e-03  1.35704605e-02\n",
      "  2.30934843e-02 -8.48830957e-03  3.41173969e-02 -5.76810986e-02\n",
      "  3.59081849e-02 -3.77130210e-02 -4.29146364e-02  1.44558726e-03\n",
      " -1.70983244e-02 -5.72382426e-03 -3.62992249e-02 -9.09234688e-04\n",
      " -2.38243211e-02  1.87087320e-02  4.38890941e-02  2.88321543e-02\n",
      "  6.21013753e-02  3.61285284e-02 -2.09323857e-02  3.02677345e-03\n",
      " -1.93318818e-02 -6.18097745e-02 -4.90837991e-02  7.28257895e-02\n",
      " -3.48043791e-03  2.43014377e-02  6.18872680e-02 -1.72251742e-02\n",
      "  5.06519526e-02  4.55476344e-03  5.39640561e-02  2.58912947e-02\n",
      " -6.22502295e-03 -5.39078973e-02 -2.53205355e-02  5.13095148e-02\n",
      " -3.17618474e-02 -4.39518318e-03 -1.90868713e-02  3.45914857e-03\n",
      " -2.60426104e-02  2.98499670e-02 -1.18754217e-02 -3.58920060e-02\n",
      "  1.42722493e-02  6.30817264e-02 -6.91877492e-03  3.73739079e-02\n",
      "  4.44779769e-02  1.39429327e-02 -4.30670334e-03 -2.04219799e-02\n",
      "  4.48438600e-02 -9.03594866e-02  4.40892950e-02 -1.06598614e-02\n",
      " -6.06912635e-02  4.15932424e-02  2.25113742e-02 -1.94298159e-02\n",
      " -4.20998335e-02  6.21341802e-02 -1.72320940e-02 -6.72030263e-03\n",
      " -7.07151461e-03 -2.83582788e-02  1.89555660e-02  4.89602499e-02\n",
      "  3.70606259e-02 -1.11291679e-02 -1.04855523e-02 -3.09351739e-02\n",
      "  3.50827836e-02 -2.56475974e-02 -3.38508971e-02  1.74775124e-02\n",
      "  4.22990741e-03  3.03625362e-03 -4.31622751e-02  8.59748945e-03\n",
      " -4.18462045e-02  5.45659987e-03 -3.26099223e-03 -8.09448026e-03\n",
      " -5.54516055e-02  4.46987785e-02 -1.90453976e-02  1.39698628e-02\n",
      " -5.40088937e-02  3.56206000e-02  3.71292830e-02 -3.56736667e-02\n",
      "  1.06689718e-03  2.06989702e-02  3.12613100e-02  4.30699103e-02\n",
      " -2.22522207e-02  2.76577845e-02 -2.65160995e-03 -6.12624325e-02\n",
      "  1.69993881e-02  1.54193491e-04  1.88353807e-02 -2.92017572e-02\n",
      "  4.43609320e-02 -3.12385187e-02  3.15267779e-02  4.51556481e-02\n",
      "  5.35688661e-02 -3.88873857e-03  1.07912580e-02 -5.67657575e-02\n",
      " -4.61167432e-02 -5.31622842e-02 -1.72436219e-02  1.54899033e-02\n",
      "  3.48329544e-02  4.56704795e-02  2.95683835e-02 -1.52469072e-02\n",
      " -1.57910287e-02 -2.35796589e-02 -1.24572963e-02 -6.88540703e-03\n",
      "  5.95519901e-04 -7.03717843e-02  3.19938734e-02 -3.77361737e-02\n",
      " -1.30920084e-02 -1.00922910e-02 -2.20087599e-02 -2.37894617e-02\n",
      "  9.13405232e-03  6.32132143e-02 -2.77765114e-02  2.03531552e-02\n",
      "  3.34673226e-02  1.49607297e-03 -4.78487723e-02 -4.95969923e-03\n",
      " -3.47176977e-02  2.74329484e-02  8.04388989e-03  4.65422533e-02\n",
      "  1.36354612e-02  3.81129980e-02 -5.43411188e-02  7.43411854e-02\n",
      "  7.33900741e-02 -4.72322397e-04 -5.85879683e-02  1.48186656e-02\n",
      "  4.38929489e-03 -2.73009576e-02 -2.76928898e-02 -1.38241546e-02\n",
      "  2.04277001e-02  2.02456489e-02  2.45736558e-02  2.29613464e-02\n",
      " -2.71269754e-02 -5.50596081e-02  1.28598185e-02  3.23994346e-02\n",
      " -3.75361517e-02  1.81147847e-02  3.76251042e-02  1.47671839e-02\n",
      " -4.28795256e-02 -2.62651853e-02  1.11830886e-02  3.39148976e-02\n",
      " -3.77765782e-02 -3.42157446e-02  2.65030302e-02 -6.05266429e-02\n",
      " -2.00414695e-02 -4.42143669e-03  3.12490854e-02 -3.19178998e-02\n",
      " -4.37364466e-02 -3.00225466e-02  5.55393705e-03 -2.26743752e-03\n",
      " -4.84678894e-02 -4.76442128e-02 -9.30550136e-03 -4.73386571e-02\n",
      " -2.58711446e-02 -8.86021368e-03 -1.18130026e-02 -3.12454924e-02\n",
      " -5.47969574e-03 -3.59599665e-02 -2.36614142e-02 -2.67687961e-02\n",
      "  3.88972014e-02  6.83987141e-02 -1.03354277e-02 -2.55972915e-03\n",
      " -8.59064907e-02  7.56977201e-02  2.52770763e-02  6.51937500e-02\n",
      "  3.22762765e-02  4.03016098e-02 -3.13418210e-02  1.47112086e-02\n",
      " -3.47638316e-02 -5.26693426e-02 -3.06851454e-02 -3.10144182e-02\n",
      " -7.07756402e-03  2.44645532e-02  1.15295025e-02 -1.60439662e-03\n",
      " -3.38952951e-02 -3.65396477e-02  2.57005822e-02  1.22827310e-02\n",
      " -1.02658952e-02 -3.69578116e-02 -2.10982729e-02 -4.55933064e-02\n",
      " -1.06730973e-02  2.28849314e-02  3.79764587e-02  7.40617588e-02\n",
      " -2.77704950e-02  6.66249096e-02  6.46527186e-02  3.74570563e-02\n",
      " -1.60000492e-02  1.08831620e-03  2.52306052e-02 -7.79910013e-02\n",
      "  7.96655007e-03 -4.12543006e-02 -3.79352123e-02 -1.09194238e-02\n",
      " -4.63443696e-02 -1.69067364e-02 -5.76607650e-04  3.14206555e-02\n",
      "  7.55409151e-02 -8.80458276e-04 -5.31493872e-03 -3.92018557e-02\n",
      "  2.75394041e-02 -2.52610631e-02  1.32690128e-02 -3.60591598e-02\n",
      "  6.10154979e-02 -1.52057568e-02 -1.82524994e-02  1.99215282e-02\n",
      "  2.57462580e-02  4.05582376e-02  1.47421483e-03 -8.34285244e-02\n",
      "  3.55679616e-02 -2.64081769e-02 -2.29014605e-02  3.08868010e-02\n",
      " -2.10349094e-02  5.02536856e-02 -4.17046659e-02  1.75501164e-02\n",
      " -1.49095226e-02 -1.76720247e-02  1.52575858e-02  4.18705605e-02\n",
      " -5.65840602e-02  6.64917976e-02  4.92083319e-02  3.61651182e-02\n",
      " -8.49300437e-03  1.34628797e-02 -1.52330399e-02  2.67027244e-02\n",
      " -2.81190164e-02  2.97403801e-02  3.28634009e-02 -2.05298816e-03\n",
      "  3.90061513e-02 -2.40165740e-02  4.62296642e-02  7.31997611e-03\n",
      "  3.28684449e-02 -6.21295273e-02  5.42134531e-02 -8.33438113e-02\n",
      "  1.45709319e-02 -3.00875138e-02  6.45120814e-02  4.09880653e-02\n",
      " -1.20292492e-02 -8.43400508e-03 -2.82781422e-02 -1.77240334e-02\n",
      "  2.21161749e-02 -9.08449385e-03 -4.59558005e-03 -2.38176845e-02\n",
      " -2.61650216e-02  3.18449922e-03 -3.42326821e-03  4.25567627e-02\n",
      "  4.06223768e-03 -5.69855655e-03 -9.10596922e-03 -6.89802738e-03\n",
      "  1.46663236e-02  3.43552567e-02 -1.41136460e-02 -3.56311202e-02\n",
      " -7.37890378e-02  6.13628216e-02 -6.54142722e-02 -4.30644043e-02\n",
      "  2.66039856e-02  6.84142159e-03 -4.39857766e-02  3.19656939e-03\n",
      " -2.20220145e-02 -3.51372734e-02 -5.02638565e-03  5.89050800e-02\n",
      "  2.97816359e-02 -7.14024454e-02 -5.48335947e-02 -1.08578224e-02\n",
      " -4.70869690e-02 -3.67407575e-02 -4.26110327e-02  1.01126672e-03\n",
      "  5.84564917e-02 -1.10437367e-02 -2.23309621e-02 -9.41562466e-03\n",
      " -6.15274580e-03 -2.62933262e-02  8.31414945e-03  6.21662056e-03\n",
      "  3.83502953e-02 -3.60830463e-02 -4.95408289e-03 -5.83890602e-02\n",
      "  1.60705030e-03  3.89689095e-02 -5.05114570e-02  6.37502596e-02\n",
      " -6.72081718e-03  4.90855018e-04 -2.37147678e-02  1.13887461e-02\n",
      " -1.15509536e-02 -3.84709500e-02  8.82295601e-04  5.86276967e-03\n",
      " -4.87968465e-03 -2.52650380e-02  4.23433036e-02  9.67789721e-03\n",
      "  1.03744743e-02 -4.73086536e-02  1.39149856e-02  6.12891093e-02\n",
      " -2.32874528e-02  1.54130952e-02  1.28279654e-02  1.12118414e-02\n",
      " -1.74361765e-01 -4.59655896e-02 -5.44875823e-02 -5.25256507e-02\n",
      "  2.99169775e-02  1.11819366e-02  1.11327609e-02 -5.22065870e-02\n",
      "  4.48205334e-04  3.56143303e-02  1.30518444e-03  4.51602135e-03\n",
      " -3.17994580e-02  4.56249807e-03  4.17426154e-02  1.33089479e-02\n",
      "  2.11951919e-02  3.46516147e-02 -6.78076744e-02 -5.66495135e-02\n",
      " -5.87123958e-03 -2.60136127e-02 -8.27712640e-02 -2.76016649e-02\n",
      " -1.05483439e-02 -1.11188442e-02  5.70711028e-03 -1.63377970e-02\n",
      "  4.82273346e-04 -5.99805750e-02  1.24411294e-02  2.68170312e-02\n",
      "  7.35775707e-03  6.69830339e-03  7.08574196e-03 -3.55017520e-02\n",
      " -1.74802449e-03  1.35258539e-02 -2.53350809e-02  2.10717507e-02\n",
      "  3.47890262e-03 -2.18691095e-03 -2.91765723e-02  6.95642922e-03\n",
      " -5.21245189e-02  1.82903633e-02  6.15929067e-02  1.98088568e-02\n",
      "  2.56563853e-02  4.96852808e-02 -4.27745916e-02 -1.49554107e-02\n",
      "  4.45085578e-02 -2.87601855e-02  1.56746022e-02  2.58538611e-02\n",
      "  2.58160792e-02 -3.88774881e-03 -4.69051152e-02 -3.14795710e-02\n",
      "  4.39151749e-02  7.98922107e-02 -2.09917314e-02 -3.36406678e-02\n",
      " -3.08549497e-02  1.13137476e-02 -9.96645726e-03 -4.45971005e-02\n",
      " -6.58214511e-03  2.42271069e-02 -3.51371579e-02 -3.77682075e-02\n",
      "  2.33592149e-02 -7.01744407e-02  6.15730602e-03 -1.97486319e-02\n",
      " -3.31535307e-03  1.79695673e-02  2.67336592e-02 -5.33065386e-03\n",
      "  1.41401011e-02 -2.63751466e-02  3.20568420e-02  4.42400426e-02\n",
      "  4.33863588e-02  1.62169300e-02 -4.56279330e-02  1.01626106e-02\n",
      " -5.75461145e-03  5.05284891e-02  2.85199042e-02 -6.34567859e-03\n",
      " -1.33422147e-02  3.28343990e-03 -1.46394726e-02  1.73383523e-02\n",
      "  8.27409979e-03  3.47786285e-02 -1.00453822e-02  3.49416435e-02\n",
      " -8.17900174e-04  5.06968088e-02  3.82611156e-02  1.09825823e-02\n",
      " -2.11396813e-02 -2.27139872e-02  1.13788350e-02  3.33675630e-02\n",
      " -5.24595007e-02 -4.70978878e-02 -1.27455210e-02  1.38229374e-02\n",
      "  1.55954538e-02  2.74229720e-02  3.69341597e-02 -2.42248513e-02\n",
      " -1.10159339e-02  2.19289213e-02  1.29518136e-02  1.78886037e-02\n",
      " -4.86836471e-02 -1.98112931e-02  3.22802225e-03 -3.64963599e-02\n",
      " -6.71206713e-02 -1.53024299e-02  1.80098973e-02 -3.92962880e-02\n",
      " -4.00632806e-02  7.51394592e-03  4.74860966e-02 -1.86592005e-02\n",
      " -4.68141818e-03  1.97507367e-02  2.77416985e-02 -4.21093451e-03\n",
      " -4.33898084e-02  2.22488660e-02  2.70002913e-02  7.27592111e-02\n",
      " -5.49758822e-02 -1.06816450e-02 -1.96431531e-03 -8.31903517e-03\n",
      "  7.19608888e-02 -9.38401744e-03 -2.67090481e-02  8.18498656e-02\n",
      "  9.92551539e-03 -4.34010737e-02  1.08449841e-02 -4.41303328e-02\n",
      " -1.57740526e-02  2.19211821e-02  2.62776110e-02  3.83341387e-02\n",
      "  5.89757487e-02 -1.21280607e-02  2.24770494e-02  8.48389580e-04\n",
      "  4.59307292e-03 -1.07351309e-02  3.44073772e-03 -2.99480297e-02\n",
      "  6.10514395e-02  6.74273670e-02 -6.70938566e-02 -4.21480127e-02\n",
      "  3.12112570e-02 -5.85565045e-02 -5.28112352e-02  2.01942995e-02\n",
      "  2.40550898e-02 -8.17653835e-02 -3.32192425e-03  7.52057955e-02\n",
      "  2.97361966e-02  1.39365792e-02 -1.89265870e-02 -1.85398618e-03\n",
      " -1.43966880e-02 -4.02394198e-02 -6.03925390e-03 -1.23736011e-02\n",
      " -5.33165131e-03  2.22193021e-02 -1.66326277e-02 -5.97378332e-03\n",
      "  3.18558142e-02 -5.75102633e-03  4.07419493e-03 -1.60697829e-02\n",
      " -4.72477190e-02  9.15343836e-02 -5.58995865e-02  1.89597216e-02\n",
      "  4.75530475e-02 -5.62414639e-02  5.12216315e-02 -1.68283880e-02\n",
      "  5.05364090e-02  8.98986459e-02 -1.73084158e-02 -3.20105702e-02\n",
      "  7.43592018e-03  3.30560631e-03 -7.75313824e-02  1.11516630e-02]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the Jina embeddings\n",
    "embeddings = SentenceTransformer('jinaai/jina-embeddings-v2-base-en')\n",
    "\n",
    "# Chunk the text\n",
    "text = \"\"\"Hi, I'm Sandhya! I love coding in Python. Python is great for data science and machine learning.\"\"\"\n",
    "\n",
    "# Single embedding\n",
    "emb = embeddings.encode(text)  # Changed from embed() to encode()\n",
    "print(f\"Embedding shape: {emb.shape}\")  # Print the shape to verify\n",
    "print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c0c5286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "Text: Natural language processing has revolutionized how we interact with computers.\n",
      "Machine learning models can now understand context, generate text, and even translate\n",
      "between languages with remarkable accuracy. This transformation has enabled applications\n",
      "ranging from virtual assistants to automated content generation.\n",
      "Token count: 49\n",
      "Start index: 0\n",
      "End index: 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayit\\codebasics_webinar\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jayit\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from chonkie import TokenChunker\n",
    "\n",
    "# Create a chunker with specific parameters\n",
    "chunker = TokenChunker(\n",
    "    tokenizer=\"gpt2\",\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=128\n",
    ")\n",
    "\n",
    "text = \"\"\"Natural language processing has revolutionized how we interact with computers.\n",
    "Machine learning models can now understand context, generate text, and even translate\n",
    "between languages with remarkable accuracy. This transformation has enabled applications\n",
    "ranging from virtual assistants to automated content generation.\"\"\"\n",
    "\n",
    "# Chunk the text\n",
    "chunks = chunker.chunk(text)\n",
    "\n",
    "# Process each chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"Text: {chunk.text}\")\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Start index: {chunk.start_index}\")\n",
    "    print(f\"End index: {chunk.end_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8537dc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text: This is the first sentence. This is the second sentence. \n",
      "And here's a third one with some additional context.\n",
      "Token count: 110\n"
     ]
    }
   ],
   "source": [
    "from chonkie import SentenceChunker\n",
    "\n",
    "# Basic initialization with default parameters\n",
    "chunker = SentenceChunker(\n",
    "    tokenizer_or_token_counter=\"character\",  # Default tokenizer (or use \"gpt2\", etc.)\n",
    "    chunk_size=2048,                  # Maximum tokens per chunk\n",
    "    chunk_overlap=128,               # Overlap between chunks\n",
    "    min_sentences_per_chunk=1        # Minimum sentences in each chunk\n",
    ")\n",
    "\n",
    "text = \"\"\"This is the first sentence. This is the second sentence. \n",
    "And here is a third one with some additional context.\"\"\"\n",
    "chunks = chunker.chunk(text)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk text: {chunk.text}\")\n",
    "    print(f\"Token count: {chunk.token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b68649f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text: This is the first sentence. This is the second sentence. \n",
      "And here's a third one with some additional context.\n",
      "Token count: 110\n"
     ]
    }
   ],
   "source": [
    "from chonkie import RecursiveChunker, RecursiveRules\n",
    "\n",
    "chunker = RecursiveChunker(\n",
    "    tokenizer_or_token_counter = \"character\",\n",
    "    chunk_size = 2048,  \n",
    ")\n",
    "\n",
    "text = \"\"\"This is the first sentence. This is the second sentence. \n",
    "And here is a third one with some additional context.\"\"\"\n",
    "\n",
    "chunks = chunker.chunk(text)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk text: {chunk.text}\")\n",
    "    print(f\"Token count: {chunk.token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfd5e907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: Your document text with multiple topics and themes...\n",
      "Tokens: 11\n"
     ]
    }
   ],
   "source": [
    "from chonkie import SemanticChunker\n",
    "\n",
    "# Initialize with semantic similarity grouping\n",
    "chunker = SemanticChunker(\n",
    "    embedding_model=\"minishlab/potion-base-32M\",\n",
    "    threshold=0.7,  # Similarity threshold\n",
    "    chunk_size=512\n",
    ")\n",
    "\n",
    "text = \"\"\"Your document text with multiple topics and themes...\"\"\"\n",
    "chunks = chunker.chunk(text)\n",
    "\n",
    "# Process chunks\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk: {chunk.text[:50]}...\")\n",
    "    print(f\"Tokens: {chunk.token_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
