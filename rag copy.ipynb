{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08b69a7",
   "metadata": {},
   "source": [
    "##### This tutorial guides you through executing the Jupyter notebook step by step, forming a complete Retrieval-Augmented Generation (RAG) pipeline. The pipeline processes a PDF document (\"Foundations of LLMs.pdf\"), extracts and chunks text, embeds and indexes chunks for retrieval, generates answers using DSPy, evaluates performance with RAGAS metrics, optimizes prompts, and traces everything with Langfuse for observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff9e78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayit\\codebasics_webinar\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import asyncio  # For potential async extensions\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Tuple, Dict\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.document import DoclingDocument\n",
    "import dspy\n",
    "from dspy import Retrieve, ChainOfThought, Predict\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "from dspy.evaluate import answer_exact_match\n",
    "# from ragatouille import RAGPretrainedModel\n",
    "from rouge_score import rouge_scorer\n",
    "import ragas\n",
    "from ragas.testset import TestsetGenerator\n",
    "from datasets import Dataset\n",
    "from dspy.evaluate import Evaluate\n",
    "import groq\n",
    "import warnings\n",
    "\n",
    "# To ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0fd417f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5400eea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse connected!\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Langfuse Setup (Enhanced for Production)\n",
    "from langfuse import Langfuse  # Use Langfuse class for client\n",
    "from langfuse import observe  # For context updates\n",
    "from langfuse import get_client  \n",
    "\n",
    "langfuse = get_client()\n",
    "# Env vars (as before)\n",
    "langfuse_client = Langfuse()  # Auto-loads from env\n",
    "if langfuse_client.auth_check():\n",
    "    print(\"Langfuse connected!\")\n",
    "else:\n",
    "    print(\"Langfuse auth failed—check env vars.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c878d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instrument DSPy\n",
    "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "DSPyInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fad6eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Docling + Late Chunking (enhanced @observe with user/session)\n",
    "@observe(name=\"Extract Full Text\")\n",
    "def extract_full_text_from_docling(doc: DoclingDocument) -> str:\n",
    "    \"\"\"\n",
    "    Extract concatenated full text from a DoclingDocument, preserving structure via simple markdown-like formatting.\n",
    "    \"\"\"\n",
    "    with langfuse_client.start_as_current_span(name=\"Extract Full Text\") as span:\n",
    "        try:\n",
    "            span.update_trace(  # Set user/session\n",
    "                user_id=\"user567\",\n",
    "                session_id=\"rag_session\"\n",
    "            )\n",
    "            text_parts = []\n",
    "            # Extract from texts (all text items: paragraphs, headings, etc.)\n",
    "            for item in doc.texts:\n",
    "                if hasattr(item, 'text') and item.text:\n",
    "                    text_parts.append(item.text)\n",
    "            # Optional: Include table text (join cells for simple extraction)\n",
    "            for table in doc.tables:\n",
    "                if hasattr(table, 'cells'):\n",
    "                    table_text = '\\n'.join([cell.text for cell in table.cells if hasattr(cell, 'text') and cell.text])\n",
    "                    text_parts.append(table_text)\n",
    "            full_text = '\\n\\n'.join(text_parts)\n",
    "            span.update(output={\"length\": len(full_text)})  # Partial output\n",
    "            return full_text\n",
    "        except Exception as e:\n",
    "            span.update(level=\"ERROR\", status_message=str(e))\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "978591af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 17:12:56,084 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-10-05 17:12:56,264 - INFO - Going to convert document batch...\n",
      "2025-10-05 17:12:56,266 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e647edf348883bed75367b22fbe60347\n",
      "2025-10-05 17:12:56,293 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-05 17:12:56,297 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-10-05 17:12:56,321 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-05 17:12:56,331 - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-10-05 17:12:56,641 - INFO - Accelerator device: 'cpu'\n",
      "2025-10-05 17:12:58,586 - INFO - Accelerator device: 'cpu'\n",
      "2025-10-05 17:13:00,039 - INFO - Accelerator device: 'cpu'\n",
      "2025-10-05 17:13:00,994 - INFO - Processing document Principles_for_the_Management_of_Credit_Risk.pdf\n",
      "2025-10-05 17:13:45,722 - INFO - Finished converting document Principles_for_the_Management_of_Credit_Risk.pdf in 49.66 sec.\n"
     ]
    }
   ],
   "source": [
    "# Parse + Chunk\n",
    "source = \"C:/Users/jayit/Downloads/finance_docs/Principles_for_the_Management_of_Credit_Risk.pdf\"\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)\n",
    "doc = result.document\n",
    "full_text = extract_full_text_from_docling(doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7422413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def late_chunk_text(text: str, model: SentenceTransformer, chunk_token_size: int = 512, overlap_tokens: int = 50, max_sequence_length: int = 8192) -> List[str]:\n",
    "    \"\"\"\n",
    "    Apply late chunking: Embed full text, then pool embeddings for overlapping chunks.\n",
    "    Args:\n",
    "        text: Full document text.\n",
    "        model: Pre-loaded embedding model.\n",
    "        chunk_token_size: Approximate tokens per chunk.\n",
    "        overlap_tokens: Tokens of overlap between chunks for smoothness.\n",
    "    Returns:\n",
    "        List of (chunk_text, chunk_embedding) tuples.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    # Convert text to sentences first to preserve some natural boundaries\n",
    "    sentences = text.split('.')\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Clean the sentence\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        # Tokenize single sentence\n",
    "        tokens = model.tokenizer.encode(\n",
    "            sentence,\n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            max_length=max_sequence_length\n",
    "        )\n",
    "        \n",
    "        sentence_length = len(tokens)\n",
    "        \n",
    "        # If adding this sentence would exceed chunk size, store current chunk and start new one\n",
    "        if current_length + sentence_length > chunk_token_size and current_chunk:\n",
    "            # Decode current chunk to text\n",
    "            chunk_text = model.tokenizer.decode(current_chunk, skip_special_tokens=True)\n",
    "            if chunk_text.strip():\n",
    "                chunks.append(chunk_text.strip())\n",
    "            # Start new chunk with overlap\n",
    "            if overlap_tokens > 0 and current_chunk:\n",
    "                current_chunk = current_chunk[-overlap_tokens:]\n",
    "                current_length = len(current_chunk)\n",
    "            else:\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "                \n",
    "        # Add tokens to current chunk\n",
    "        current_chunk.extend(tokens)\n",
    "        current_length += sentence_length\n",
    "        \n",
    "        # If current chunk exceeds max length, force split\n",
    "        while current_length > chunk_token_size:\n",
    "            chunk_tokens = current_chunk[:chunk_token_size]\n",
    "            chunk_text = model.tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "            if chunk_text.strip():\n",
    "                chunks.append(chunk_text.strip())\n",
    "            # Keep overlap for next chunk\n",
    "            current_chunk = current_chunk[chunk_token_size-overlap_tokens:]\n",
    "            current_length = len(current_chunk)\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_chunk:\n",
    "        chunk_text = model.tokenizer.decode(current_chunk, skip_special_tokens=True)\n",
    "        if chunk_text.strip() and chunk_text not in chunks:\n",
    "            chunks.append(chunk_text.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "494d5c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 17:13:45,771 - INFO - Use pytorch device_name: cpu\n",
      "2025-10-05 17:13:45,772 - INFO - Load pretrained SentenceTransformer: jinaai/jina-embeddings-v2-base-en\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 66\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('jinaai/jina-embeddings-v2-base-en')\n",
    "chunk_texts = late_chunk_text(\n",
    "            full_text, \n",
    "            model, \n",
    "            chunk_token_size=256,  # Smaller chunks\n",
    "            overlap_tokens=25,     # Proportionally smaller overlap\n",
    "            max_sequence_length=8192\n",
    "        )\n",
    "print(f\"Total chunks created: {len(chunk_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d184a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "class SimpleRetriever(dspy.Retrieve):\n",
    "    def __init__(self, chunk_texts: List[str], model: SentenceTransformer, k: int = 3):\n",
    "        self.chunks = chunk_texts\n",
    "        self.model = model\n",
    "        self.k = k\n",
    "        # Pre-compute embeddings\n",
    "        self.chunk_embeddings = self.model.encode(chunk_texts, convert_to_tensor=True)\n",
    "    \n",
    "    def forward(self, query: str) -> dspy.Example:\n",
    "        query_emb = self.model.encode([query], convert_to_tensor=True)\n",
    "        scores = util.pytorch_cos_sim(query_emb, self.chunk_embeddings)[0]\n",
    "        top_k = torch.topk(scores, k=self.k)\n",
    "        passages_with_idx = [\n",
    "            {'text': self.chunks[idx], 'index': idx.item()}\n",
    "            for idx in top_k.indices\n",
    "        ]\n",
    "        return dspy.Example(passages=passages_with_idx).with_inputs('question')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4650b4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/3 [00:00<?, ?it/s]BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Batches: 100%|██████████| 3/3 [00:15<00:00,  5.15s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 Retrieved Chunks:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk #1:\n",
      "Text: to reduce dependency on a 11 see footnote 5 21 credit risk management particular sector of the economy or group of related borrowers banks must be careful not to enter into transactions with borrowers...\n",
      "Index: 42\n",
      "\n",
      "Chunk #2:\n",
      "Text: they are able to make sound credit decisions consistent with their credit strategy and meet competitive time, pricing and structuring pressures 44 each credit proposal should be subject to careful ana...\n",
      "Index: 29\n",
      "\n",
      "Chunk #3:\n",
      "Text: capable of conducting the activity to the highest standards and in compliance with the bank's policies and procedures 10 credit risk management iii operating under a sound credit granting process prin...\n",
      "Index: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retriever = SimpleRetriever(chunk_texts, model, k=3)\n",
    "# Test retriever with a sample question\n",
    "question = \"What are the key principles of credit risk management?\"\n",
    "retrieved = retriever(question)\n",
    "\n",
    "# Print the top 3 retrieved chunks with their scores\n",
    "print(\"\\nTop 3 Retrieved Chunks:\")\n",
    "print(\"-\" * 80)\n",
    "for i, passage in enumerate(retrieved.passages, 1):\n",
    "    print(f\"\\nChunk #{i}:\")\n",
    "    print(f\"Text: {passage['text'][:200]}...\")  # Show first 200 chars\n",
    "    print(f\"Index: {passage['index']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ece3ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 Retrieved Chunks:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk #1:\n",
      "Text: should be adequate checks and balances in place to promote sound credit decisions 8 credit risk management complexity of the bank's activities the policies should be designed and implemented within the context of internal and external factors such as the bank's market position, trade area, staff capabilities and technology policies and procedures that are properly developed and implemented enable the bank to : ( i ) maintain sound credit - granting standards ; ( ii ) monitor and control credit risk ; ( iii ) properly evaluate new business opportunities ; and ( iv ) identify and administer problem credits 19 as discussed further in paragraphs 30 and 37 through 41 below, banks should develop and implement policies and procedures to ensure that the credit portfolio is adequately diversified given the bank's target markets and overall credit strategy in particular, such policies should establish targets for portfolio mix as well as set exposure limits on single counterparties and groups of connected counterparties, particular industries or economic sectors, geographic regions and specific products banks should ensure that their own internal exposure limits comply with any prudential limits or restrictions set by the banking supervisors 20 in order to be effective, credit policies must be communicated throughout the organisation, implemented through appropriate procedures, monitored and periodically revised to take into account changing internal and external circumstances...\n",
      "Index: 15\n",
      "\n",
      "Chunk #2:\n",
      "Text: credit risk models are used supervisors should also review the results of any independent internal reviews of the credit - granting and credit administration functions supervisors should also make use of any reviews conducted by the bank's external auditors, where available 83 supervisors should take particular note of whether bank management recognises problem credits at an early stage and takes the appropriate actions 14 supervisors should monitor trends within a bank's overall credit portfolio and discuss with senior management any marked deterioration supervisors should also assess whether the capital of the bank, in addition to its provisions and reserves, is adequate related to the level of credit risk identified and inherent in the bank's various on - and off - balance sheet activities 84 in reviewing the adequacy of the credit risk management process, home country supervisors should also determine that the process is effective across business lines, subsidiaries and national boundaries it is important that supervisors evaluate the credit risk management system not only at the level of individual businesses or legal entities but also across the wide spectrum of activities and subsidiaries within the consolidated banking organisation 85...\n",
      "Index: 50\n",
      "\n",
      "Chunk #3:\n",
      "Text: sheet 37 an important element of credit risk management is the establishment of exposure limits on single counterparties and groups of connected counterparties such limits are frequently based in part on the internal risk rating assigned to the borrower or counterparty, with counterparties assigned better risk ratings having potentially higher exposure limits limits should also be established for particular industries or economic sectors, geographic regions and specific products 38 exposure limits are needed in all areas of the bank's activities that involve credit risk these limits help to ensure that the bank's credit - granting activities are adequately diversified as mentioned earlier, much of the credit exposure faced by some banks comes from activities and instruments in the trading book and off the balance sheet limits on such transactions are particularly effective in managing the overall credit risk profile or counterparty risk of a bank in order to be effective, limits should generally be binding and not driven by customer demand 39 effective measures of potential future exposure are essential for the establishment of meaningful limits, placing an upper bound on the overall scale of activity with, and exposure to, a given counterparty, based on a comparable measure of exposure across a bank's various activities ( both on and off - balance - sheet ) 40 banks should consider the results of stress testing in the overall limit setting and monitoring process...\n",
      "Index: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test retriever with a sample question\n",
    "question = \"What are the key factors to consider in credit risk assessment and monitoring?\"\n",
    "retrieved = retriever(question)\n",
    "\n",
    "# Print the top 3 retrieved chunks with their scores\n",
    "print(\"\\nTop 3 Retrieved Chunks:\")\n",
    "print(\"-\" * 80)\n",
    "for i, passage in enumerate(retrieved.passages, 1):\n",
    "    print(f\"\\nChunk #{i}:\")\n",
    "    print(f\"Text: {passage['text']}...\")  # Show first 200 chars\n",
    "    print(f\"Index: {passage['index']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c06ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, retriever: SimpleRetriever, num_passages=3):\n",
    "        super().__init__()\n",
    "        self.retrieve = Retrieve(k=num_passages)\n",
    "        # Fix the signature format\n",
    "        self.generate_answer = ChainOfThought(\"context, question -> answer\")\n",
    "        self.retriever = retriever\n",
    "    \n",
    "    def forward(self, question: str):\n",
    "        retrieved = self.retriever(question)\n",
    "        passages = retrieved.passages\n",
    "        passage_texts = [p['text'] for p in passages]\n",
    "        prediction = self.generate_answer(             \n",
    "            passages=str(passage_texts),\n",
    "            question=question,\n",
    "        )\n",
    "        prediction.retrieved_indices = [p['index'] for p in passages]\n",
    "        return dspy.Prediction(answer=prediction.answer, retrieved_indices=prediction.retrieved_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6276db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv(\"LLM_API_KEY\")\n",
    "lm = dspy.LM('qwen/qwen3-32b', api_key=groq_api_key, api_base='https://api.groq.com/openai/v1')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d914ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d808a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing RAG Pipeline ===\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question: What are the key components of credit risk management?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.35it/s]\n",
      "2025/10/05 17:14:07 WARNING dspy.predict.predict: Not all input fields were provided to module. Present: ['question']. Missing: ['context'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The key components of credit risk management are:  \n",
      "1. **Credit Assessment**: Evaluating borrowers' creditworthiness using financial analysis, credit scores, and scoring models.  \n",
      "2. **Credit Limits**: Establishing appropriate credit thresholds based on risk appetite and borrower capacity.  \n",
      "3. **Monitoring & Review**: Continuously tracking borrower performance and financial health.  \n",
      "4. **Risk Mitigation**: Using collateral, guarantees, or insurance to reduce potential losses.  \n",
      "5. **Diversification**: Spreading credit exposure across industries, geographies, and borrower types.  \n",
      "6. **Recovery Processes**: Implementing strategies for debt restructuring or legal recovery in case of default.  \n",
      "7. **Governance & Compliance**: Ensuring adherence to internal policies and regulatory requirements.  \n",
      "8. **Stress Testing**: Simulating adverse scenarios to assess portfolio resilience.\n",
      "\n",
      "Retrieved Passage Indices: [53, 15, 57]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question: How should banks monitor and control credit risk?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.98it/s]\n",
      "2025/10/05 17:14:08 WARNING dspy.predict.predict: Not all input fields were provided to module. Present: ['question']. Missing: ['context'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Banks should monitor and control credit risk by employing credit scoring models, diversifying loan portfolios, conducting stress tests, enforcing clear credit policies, and continuously tracking borrower performance. They should also utilize early warning systems, manage collateral effectively, comply with regulatory requirements, and leverage risk transfer tools like credit derivatives to mitigate potential losses.\n",
      "\n",
      "Retrieved Passage Indices: [29, 10, 24]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question: What are the best practices for credit portfolio management?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.33it/s]\n",
      "2025/10/05 17:14:08 WARNING dspy.predict.predict: Not all input fields were provided to module. Present: ['question']. Missing: ['context'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The best practices for credit portfolio management include:  \n",
      "1. **Diversification**: Spread risk across industries, geographies, and borrower types to avoid overexposure.  \n",
      "2. **Credit Risk Assessment**: Use robust underwriting standards, credit scoring models, and financial ratio analysis to evaluate borrower creditworthiness.  \n",
      "3. **Continuous Monitoring**: Track delinquencies, defaults, and macroeconomic indicators to identify early warning signs.  \n",
      "4. **Stress Testing**: Simulate adverse scenarios (e.g., recession, market crashes) to assess portfolio resilience.  \n",
      "5. **Liquidity Management**: Maintain a balance between short-term and long-term assets to meet obligations during downturns.  \n",
      "6. **Regulatory Compliance**: Adhere to capital adequacy, reporting, and lending regulations (e.g., Basel III, local laws).  \n",
      "7. **Risk Mitigation Tools**: Utilize credit derivatives (e.g., CDS), collateral requirements, and loan covenants to reduce exposure.  \n",
      "8. **Technology Integration**: Leverage AI/ML for predictive analytics, real-time monitoring, and fraud detection.  \n",
      "9. **Governance Frameworks**: Establish clear risk appetite statements, board oversight, and internal audit processes.  \n",
      "10. **Active Portfolio Rebalancing**: Adjust holdings based on performance, market conditions, and strategic goals.  \n",
      "\n",
      "These practices ensure a resilient, profitable credit portfolio while minimizing systemic risks.\n",
      "\n",
      "Retrieved Passage Indices: [8, 53, 36]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test RAG pipeline with sample questions\n",
    "test_questions = [\n",
    "    \"What are the key components of credit risk management?\",\n",
    "    \"How should banks monitor and control credit risk?\",\n",
    "    \"What are the best practices for credit portfolio management?\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Testing RAG Pipeline ===\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    prediction = rag(question=question)\n",
    "    print(\"\\nAnswer:\", prediction.answer)\n",
    "    print(\"\\nRetrieved Passage Indices:\", prediction.retrieved_indices)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4a0bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Metrics (enhanced to store scores in Langfuse for analytics)\n",
    "def mrr_metric(example: dspy.Example, pred: dspy.Prediction, trace=None) -> float:\n",
    "    if not hasattr(example, 'gold_relevant_indices') or not example.gold_relevant_indices:\n",
    "        return 0.0\n",
    "    gold_set = set(example.gold_relevant_indices)\n",
    "    relevant_ranks = [rank + 1 for rank, idx in enumerate(pred.retrieved_indices) if idx in gold_set]\n",
    "    score = 1.0 / relevant_ranks[0] if relevant_ranks else 0.0\n",
    "    # Store as score (requires trace_id from context; for demo, assume accessible via langfuse_context)\n",
    "    if 'trace_id' in locals():  # Placeholder; in prod, pass trace_id\n",
    "        langfuse_client.score(name=\"MRR\", value=score, trace_id=trace_id)\n",
    "    return score\n",
    "\n",
    "def rouge_l_metric(example: dspy.Example, pred: dspy.Prediction, trace=None) -> float:\n",
    "    if not hasattr(example, 'answer') or not hasattr(pred, 'answer'):\n",
    "        return 0.0\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(example.answer, pred.answer)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "def faithfulness_metric(example: dspy.Example, pred: dspy.Prediction, trace=None) -> float:\n",
    "    if not hasattr(pred, 'answer') or not hasattr(example, 'passages'):\n",
    "        return 0.0\n",
    "    passages_str = str([p['text'] for p in example.passages]) if hasattr(example, 'passages') else \"\"\n",
    "    prompt = f\"Passages: {passages_str}\\nAnswer: {pred.answer}\\nIs the answer fully grounded in the passages without hallucination? Answer 'yes' or 'no'.\"\n",
    "    with dspy.context(lm=dspy.settings.lm):\n",
    "        judgment = dspy.Predict(\"prompt -> judgment\").forward(prompt=prompt).judgment.strip().lower()\n",
    "    return 1.0 if 'yes' in judgment else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8845f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Real Dev Set with Ragas (wrap generation in @observe)\n",
    "@observe()  # Traces QA generation as a span\n",
    "def generate_ragas_devset(chunk_texts: List[str], embedding_model: SentenceTransformer, num_examples: int = 10) -> List[dspy.Example]:\n",
    "    contexts = [{'context': chunk} for chunk in chunk_texts[:20]]\n",
    "    dataset = Dataset.from_list(contexts)\n",
    "    \n",
    "    generator = TestsetGenerator(llm=lm, embedding_model=model)\n",
    "    testset = generator.generate_with_langchain_docs(dataset, testset_size=num_examples)\n",
    "    \n",
    "    examples = []\n",
    "    for row in testset.to_pandas().iterrows():\n",
    "        question = row[1]['question']\n",
    "        answer = row[1]['answer']\n",
    "        \n",
    "        q_emb = embedding_model.encode([question])\n",
    "        chunk_embs = embedding_model.encode(chunk_texts)\n",
    "        similarities = np.dot(chunk_embs, q_emb.T).flatten()\n",
    "        top_indices = np.argsort(similarities)[-3:][::-1]\n",
    "        \n",
    "        ex = dspy.Example(question=question, answer=answer, \n",
    "                          gold_relevant_indices=list(top_indices)).with_inputs('question')\n",
    "        examples.append(ex)\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b6de095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Combined Metric (unchanged)\n",
    "def combined_metric(example: dspy.Example, pred: dspy.Prediction, trace=None) -> float:\n",
    "    return mrr_metric(example, pred) + rouge_l_metric(example, pred) + answer_exact_match(example, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e9a62",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# devset = generate_ragas_devset(chunk_texts, model, num_examples=10)\n",
    "        \n",
    "# # Evaluations (store scores as above)\n",
    "# evaluators = {\n",
    "#             'MRR': Evaluate(devset=devset, metric=mrr_metric, num_threads=10),\n",
    "#             'ROUGE-L': Evaluate(devset=devset, metric=rouge_l_metric, num_threads=10),\n",
    "#             'Exact Match': Evaluate(devset=devset, metric=answer_exact_match, num_threads=10),\n",
    "#             'Faithfulness': Evaluate(devset=devset, metric=faithfulness_metric, num_threads=10)\n",
    "#         }\n",
    "        \n",
    "# baseline_scores = {name: eval_(rag) for name, eval_ in evaluators.items()}\n",
    "# print(\"\\n=== Baseline Scores ===\")\n",
    "# for name, score in baseline_scores.items():\n",
    "#     print(f\"{name}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffaaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating development set...\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "from typing import Dict, Any\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def evaluate_metric_thread(args: Dict[str, Any]) -> tuple:\n",
    "    \"\"\"\n",
    "    Helper function to evaluate a single metric in a thread\n",
    "    \"\"\"\n",
    "    name, evaluator, rag_model = args['name'], args['evaluator'], args['rag']\n",
    "    try:\n",
    "        score = evaluator(rag_model)\n",
    "        return (name, score)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {name}: {str(e)}\")\n",
    "        return (name, 0.0)\n",
    "\n",
    "def parallel_evaluate(evaluators: Dict, rag_model, max_workers: int = 4) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate metrics in parallel using ThreadPoolExecutor\n",
    "    \"\"\"\n",
    "    tasks = [\n",
    "        {\n",
    "            'name': name,\n",
    "            'evaluator': evaluator,\n",
    "            'rag': rag_model\n",
    "        }\n",
    "        for name, evaluator in evaluators.items()\n",
    "    ]\n",
    "    \n",
    "    scores = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(evaluate_metric_thread, task) for task in tasks]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            name, score = future.result()\n",
    "            scores[name] = score\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Generate dev set\n",
    "print(\"Generating development set...\")\n",
    "devset = generate_ragas_devset(chunk_texts, model, num_examples=10)\n",
    "\n",
    "# Setup evaluators\n",
    "print(\"\\nSetting up evaluators...\")\n",
    "evaluators = {\n",
    "    # 'MRR': Evaluate(devset=devset, metric=mrr_metric, num_threads=10),\n",
    "    # 'ROUGE-L': Evaluate(devset=devset, metric=rouge_l_metric, num_threads=10),\n",
    "    'Exact Match': Evaluate(devset=devset, metric=answer_exact_match, num_threads=10),\n",
    "    # 'Faithfulness': Evaluate(devset=devset, metric=faithfulness_metric, num_threads=10)\n",
    "}\n",
    "\n",
    "# Run parallel evaluation for baseline\n",
    "print(\"\\nEvaluating baseline scores in parallel...\")\n",
    "baseline_scores = parallel_evaluate(evaluators, rag, max_workers=4)\n",
    "\n",
    "print(\"\\n=== Baseline Scores ===\")\n",
    "for name, score in baseline_scores.items():\n",
    "    print(f\"{name}: {score:.3f}\")\n",
    "\n",
    "# # Run optimization\n",
    "# print(\"\\nOptimizing RAG model...\")\n",
    "# optimizer = BootstrapFewShot(metric=combined_metric)\n",
    "# optimized_rag = optimizer.compile(rag, trainset=devset)\n",
    "\n",
    "# # Run parallel evaluation for optimized model\n",
    "# print(\"\\nEvaluating optimized scores in parallel...\")\n",
    "# optimized_scores = parallel_evaluate(evaluators, optimized_rag, max_workers=4)\n",
    "\n",
    "# print(\"\\n=== Optimized Scores ===\")\n",
    "# for name, score in optimized_scores.items():\n",
    "#     gain = optimized_scores[name] - baseline_scores[name]\n",
    "#     print(f\"{name}: {score:.3f} (gain: {gain:+.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Analytics Export Example (post-eval)\n",
    "def export_analytics(client: Langfuse, session_id: str = \"rag_session\"):\n",
    "    \"\"\"Query traces via SDK and compute custom analytics (e.g., avg MRR).\"\"\"\n",
    "    # Fetch traces for session (per Metrics API patterns)\n",
    "    traces = client.get_traces(session_id=session_id, limit=100)  # SDK query\n",
    "    mrr_scores = []\n",
    "    for trace in traces:\n",
    "        # Assume scores attached; extract via trace.scores\n",
    "        mrr_score = next((s.value for s in trace.scores if s.name == \"MRR\"), 0.0)\n",
    "        mrr_scores.append(mrr_score)\n",
    "    avg_mrr = np.mean(mrr_scores) if mrr_scores else 0.0\n",
    "    print(f\"Avg MRR across {len(traces)} traces: {avg_mrr:.3f}\")\n",
    "    # For advanced: Use Metrics API via HTTP (e.g., POST /api/public/projects/{project_id}/metrics/query)\n",
    "    # with dimensions=[\"session_id\"], metrics=[\"avg(MRR)\"], filters={\"session_id\": session_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c77f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimization\n",
    "# optimizer = BootstrapFewShot(metric=combined_metric)\n",
    "# optimized_rag = optimizer.compile(rag, trainset=devset)\n",
    "        \n",
    "# optimized_scores = {name: eval_(optimized_rag) for name, eval_ in evaluators.items()}\n",
    "# print(\"\\n=== Optimized Scores ===\")\n",
    "# for name, score in optimized_scores.items():\n",
    "#     gain = optimized_scores[name] - baseline_scores[name]\n",
    "#     print(f\"{name}: {score:.3f} (gain: {gain:+.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3185614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Query\n",
    "# question = \"What is the main topic of the document?\"\n",
    "# prediction = optimized_rag(question=question)\n",
    "# print(f\"\\nAnswer: {prediction.answer}\")\n",
    "        \n",
    "# # Production Flush: Send batched data\n",
    "langfuse_client.flush()\n",
    "        \n",
    "# # Analytics Export\n",
    "export_analytics(langfuse_client, \"rag_session\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
